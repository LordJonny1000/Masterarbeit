{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:17:52.941750Z",
     "start_time": "2025-09-21T20:17:52.926577Z"
    }
   },
   "source": [
    "from importlib.metadata import metadata\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.corpus import wordnet as wn\n",
    "from deep_translator import GoogleTranslator\n",
    "from typing import Dict, List\n",
    "import time\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "class MultilingualEmbeddingExtractor:\n",
    "    def __init__(self, embeddings_path: str = Path(\"../data/numberbatch-19.08.txt\")):\n",
    "        self.embeddings_path = Path(embeddings_path)\n",
    "\n",
    "        self.target_langs = ['de', 'fr', 'es', 'it', 'pt', 'nl', 'pl', 'ru', \n",
    "                        'ja', 'ko', 'zh-CN', 'ar', 'hi', 'tr', 'sv']\n",
    "\n",
    "    def get_translations(self, word: str, source_lang: str = 'en', \n",
    "                        target_langs: List[str] = None) -> Dict[str, str]:\n",
    "        \n",
    "        if target_langs is None:\n",
    "            target_langs = self.target_langs[:10]\n",
    "        \n",
    "        translations = {source_lang: word}\n",
    "        \n",
    "        for lang in target_langs:\n",
    "            try:\n",
    "                translator = GoogleTranslator(source=source_lang, target=lang)\n",
    "                result = translator.translate(word)\n",
    "                if result:\n",
    "                    translations[lang] = result.lower()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"Error translating to {lang}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return translations\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    def load_embeddings_for_words(self, translations: Dict[str, str]) -> Dict[Tuple[str, str], np.ndarray]:\n",
    "        embeddings = {}\n",
    "        translations_lower = {lang: word.lower() for lang, word in translations.items()}\n",
    "\n",
    "        if not self.embeddings_path.exists():\n",
    "            raise FileNotFoundError(f\"Embeddings file not found: {self.embeddings_path}\")\n",
    "\n",
    "        with open(self.embeddings_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)\n",
    "\n",
    "            for line in tqdm(f, desc=\"Searching embeddings\", file=sys.stdout):\n",
    "                parts = line.rstrip().split(' ')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                entry = parts[0]\n",
    "                entry_parts = entry.split('/')\n",
    "\n",
    "                if len(entry_parts) >= 4:\n",
    "                    target_lang = entry_parts[2]\n",
    "                    target_word = entry_parts[3].lower()\n",
    "\n",
    "                    if target_lang in translations_lower and translations_lower[target_lang] == target_word:\n",
    "                        vector = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
    "                        embeddings[(target_lang, translations[target_lang])] = vector\n",
    "\n",
    "                        if len(embeddings) == len(translations):\n",
    "                            break\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_multilingual_embeddings(self, word: str, source_lang: str = \"en\") -> Dict[Tuple[str, str], np.ndarray]:\n",
    "        translations = self.get_translations(word, source_lang)\n",
    "        if not translations:\n",
    "            return {}\n",
    "\n",
    "        print(f\"Found translations for '{word}': {translations}\")\n",
    "        embeddings = self.load_embeddings_for_words(translations)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def save_to_csv(self, word, embeddings):\n",
    "        with open(Path(f\"../data/embeddings/{word}.csv\"), 'w', newline='') as file:\n",
    "            for line in embeddings.items():\n",
    "                mdata, emb = line[0], line [1]\n",
    "                writer = csv.writer(file, delimiter=\";\")\n",
    "                row = (*mdata, *[num for num in emb])\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        \n",
    "def analyze_embedding_consistency(embeddings: Dict[Tuple[str, str], np.ndarray]) -> Dict[str, float]:\n",
    "    if len(embeddings) < 2:\n",
    "        return {}\n",
    "\n",
    "    vectors = list(embeddings.values())\n",
    "    mean_vector = np.mean(vectors, axis=0)\n",
    "\n",
    "    consistencies = {}\n",
    "    for (lang, word), vector in embeddings.items():\n",
    "        cosine_sim = np.dot(vector, mean_vector) / (np.linalg.norm(vector) * np.linalg.norm(mean_vector))\n",
    "        consistencies[f\"{lang}:{word}\"] = float(cosine_sim)\n",
    "\n",
    "    return consistencies\n",
    "\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T22:05:23.500241Z",
     "start_time": "2025-09-21T22:02:55.383298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word = \"cat\"\n",
    "\n",
    "extractor = MultilingualEmbeddingExtractor()\n",
    "translations = extractor.get_translations(word)\n",
    "embeddings = extractor.load_embeddings_for_words(translations)\n",
    "extractor.save_to_csv(word, embeddings)\n"
   ],
   "id": "e4f96c41c366f420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching embeddings: 7758151it [02:21, 54806.95it/s]\n",
      "('de', 'idee')\n",
      "('en', 'idea')\n",
      "('es', 'idea')\n",
      "('fr', 'idée')\n",
      "('it', 'idea')\n",
      "('ja', 'アイデア')\n",
      "('ko', '아이디어')\n",
      "('nl', 'idee')\n",
      "('pl', 'pomysł')\n",
      "('pt', 'ideia')\n",
      "('ru', 'идея')\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:58:31.274601Z",
     "start_time": "2025-09-21T20:58:31.257931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "data = dict()\n",
    "for file in os.listdir(\"../data/embeddings\"):\n",
    "    with open(Path(\"../data/embeddings\") / Path(file), \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data[file.removesuffix('.csv')] = dict()\n",
    "        for row in reader:\n",
    "            parsed = row[0].split(';')\n",
    "            \n",
    "            data[file.removesuffix('.csv')][(parsed[0], parsed[1])] = np.array(parsed[2:], dtype=np.float32)\n",
    "            \n",
    "           \n",
    "\n",
    "#print(data['cow'])\n",
    "#print([x.shape for x in vectors])\n",
    "mean_v_1 = np.mean(list(data['sword'].values()), axis=0)\n",
    "mean_v_2 = np.mean(list(data['sheep'].values()), axis=0)\n",
    "cosine_sim = np.dot(mean_v_1, mean_v_2) / (np.linalg.norm(mean_v_1) * np.linalg.norm(mean_v_2))\n",
    "print(cosine_sim)\n"
   ],
   "id": "39d4c22d41fa4ffb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01663858\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T16:47:06.455163Z",
     "start_time": "2025-09-22T16:43:51.527152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_supersense(word, pos='n'):\n",
    "    \"\"\"\n",
    "    Holt den Supersense für ein Wort\n",
    "    pos: 'n' für Nomen, 'v' für Verben, etc.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    if synsets:\n",
    "        # Nimm das erste (häufigste) Synset\n",
    "        return [x.lexname() for x in synsets]\n",
    "        \n",
    "        #return lexname\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    word = input(\"Konzept suchen:\")\n",
    "    print(f\"{word}: {get_supersense(word)}\")"
   ],
   "id": "2b573d528247871d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone: ['noun.object', 'noun.artifact', 'noun.substance', 'noun.substance', 'noun.quantity', 'noun.plant', 'noun.person', 'noun.person', 'noun.person', 'noun.person', 'noun.person', 'noun.person', 'noun.attribute']\n",
      "stein: ['noun.artifact', 'noun.person']\n",
      "snake: ['noun.animal', 'noun.person', 'noun.object', 'noun.object', 'noun.artifact']\n",
      "bill: ['noun.communication', 'noun.communication', 'noun.possession', 'noun.act', 'noun.communication', 'noun.communication', 'noun.communication', 'noun.artifact', 'noun.artifact', 'noun.animal']\n",
      "horse: ['noun.animal', 'noun.artifact', 'noun.group', 'noun.artifact', 'noun.artifact']\n",
      "guitar: ['noun.artifact']\n",
      "baby: ['noun.person', 'noun.person', 'noun.person', 'noun.person', 'noun.person', 'noun.animal', 'noun.act']\n",
      "star: ['noun.object', 'noun.person', 'noun.object', 'noun.person', 'noun.shape', 'noun.person', 'noun.communication', 'noun.cognition']\n",
      "tiger: ['noun.person', 'noun.animal']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     word = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mKonzept suchen:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mget_supersense(word)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Schreibtisch/Multiling/Masterarbeit/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1275\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1273\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1275\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1276\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1277\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1278\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1279\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1280\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Schreibtisch/Multiling/Masterarbeit/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1320\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1317\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1318\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1319\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1321\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1322\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T16:26:36.885815Z",
     "start_time": "2025-09-22T16:26:36.881971Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a4d632463d8e470f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "62feed2ab03357dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
