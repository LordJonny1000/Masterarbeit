{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:52:40.972888Z",
     "start_time": "2025-09-21T19:52:40.948659Z"
    }
   },
   "source": [
    "from importlib.metadata import metadata\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from nltk.corpus import wordnet as wn\n",
    "from deep_translator import GoogleTranslator\n",
    "from typing import Dict, List\n",
    "import time\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "class MultilingualEmbeddingExtractor:\n",
    "    def __init__(self, embeddings_path: str = Path(\"../data/numberbatch-19.08.txt\")):\n",
    "        self.embeddings_path = Path(embeddings_path)\n",
    "\n",
    "        self.target_langs = ['de', 'fr', 'es', 'it', 'pt', 'nl', 'pl', 'ru', \n",
    "                        'ja', 'ko', 'zh-CN', 'ar', 'hi', 'tr', 'sv']\n",
    "\n",
    "    def get_translations(self, word: str, source_lang: str = 'en', \n",
    "                        target_langs: List[str] = None) -> Dict[str, str]:\n",
    "        \n",
    "        if target_langs is None:\n",
    "            target_langs = self.target_langs[:10]\n",
    "        \n",
    "        translations = {source_lang: word}\n",
    "        \n",
    "        for lang in target_langs:\n",
    "            try:\n",
    "                translator = GoogleTranslator(source=source_lang, target=lang)\n",
    "                result = translator.translate(word)\n",
    "                if result:\n",
    "                    translations[lang] = result.lower()\n",
    "                time.sleep(0.3)\n",
    "            except Exception as e:\n",
    "                print(f\"Error translating to {lang}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return translations\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    def load_embeddings_for_words(self, translations: Dict[str, str]) -> Dict[Tuple[str, str], np.ndarray]:\n",
    "        embeddings = {}\n",
    "        translations_lower = {lang: word.lower() for lang, word in translations.items()}\n",
    "\n",
    "        if not self.embeddings_path.exists():\n",
    "            raise FileNotFoundError(f\"Embeddings file not found: {self.embeddings_path}\")\n",
    "\n",
    "        with open(self.embeddings_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)\n",
    "\n",
    "            for line in tqdm(f, desc=\"Searching embeddings\", file=sys.stdout):\n",
    "                parts = line.rstrip().split(' ')\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                entry = parts[0]\n",
    "                entry_parts = entry.split('/')\n",
    "\n",
    "                if len(entry_parts) >= 4:\n",
    "                    target_lang = entry_parts[2]\n",
    "                    target_word = entry_parts[3].lower()\n",
    "\n",
    "                    if target_lang in translations_lower and translations_lower[target_lang] == target_word:\n",
    "                        vector = np.array([float(x) for x in parts[1:]], dtype=np.float32)\n",
    "                        embeddings[(target_lang, translations[target_lang])] = vector\n",
    "\n",
    "                        if len(embeddings) == len(translations):\n",
    "                            break\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def get_multilingual_embeddings(self, word: str, source_lang: str = \"en\") -> Dict[Tuple[str, str], np.ndarray]:\n",
    "        translations = self.get_translations(word, source_lang)\n",
    "        if not translations:\n",
    "            return {}\n",
    "\n",
    "        print(f\"Found translations for '{word}': {translations}\")\n",
    "        embeddings = self.load_embeddings_for_words(translations)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def save_to_csv(self, word, embeddings):\n",
    "        with open(Path(f\"../data/embeddings/{word}.csv\"), 'w', newline='') as file:\n",
    "            for line in embeddings.items():\n",
    "                mdata, emb = line[0], line [1]\n",
    "                print(mdata)\n",
    "                writer = csv.writer(file, delimiter=\";\")\n",
    "                row = (*mdata, *[num for num in emb])\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        \n",
    "def analyze_embedding_consistency(embeddings: Dict[Tuple[str, str], np.ndarray]) -> Dict[str, float]:\n",
    "    if len(embeddings) < 2:\n",
    "        return {}\n",
    "\n",
    "    vectors = list(embeddings.values())\n",
    "    mean_vector = np.mean(vectors, axis=0)\n",
    "\n",
    "    consistencies = {}\n",
    "    for (lang, word), vector in embeddings.items():\n",
    "        cosine_sim = np.dot(vector, mean_vector) / (np.linalg.norm(vector) * np.linalg.norm(mean_vector))\n",
    "        consistencies[f\"{lang}:{word}\"] = float(cosine_sim)\n",
    "\n",
    "    return consistencies\n",
    "\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T19:54:59.794560Z",
     "start_time": "2025-09-21T19:52:42.719616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word = \"sword\"\n",
    "\n",
    "extractor = MultilingualEmbeddingExtractor()\n",
    "translations = extractor.get_translations(word)\n",
    "embeddings = extractor.load_embeddings_for_words(translations)\n",
    "extractor.save_to_csv(word, embeddings)\n"
   ],
   "id": "e4f96c41c366f420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching embeddings: 7813562it [02:10, 59957.50it/s]\n",
      "('de', 'schwert')\n",
      "('en', 'sword')\n",
      "('es', 'espada')\n",
      "('fr', 'épée')\n",
      "('it', 'spada')\n",
      "('ja', '剣')\n",
      "('ko', '검')\n",
      "('nl', 'zwaard')\n",
      "('pl', 'miecz')\n",
      "('pt', 'espada')\n",
      "('ru', 'меч')\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:15:08.346664Z",
     "start_time": "2025-09-21T20:15:08.255002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "data = dict()\n",
    "for file in os.listdir(\"../data/embeddings\"):\n",
    "    with open(Path(\"../data/embeddings\") / Path(file), \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data[file.removesuffix('.csv')] = dict()\n",
    "        for row in reader:\n",
    "            parsed = row[0].split(';')\n",
    "            \n",
    "            data[file.removesuffix('.csv')][(parsed[0], parsed[1])] = np.array(parsed[2:])\n",
    "            \n",
    "           \n",
    "\n",
    "#print(data['cow'])\n",
    "vectors = list(data['cow'].values())\n",
    "#print([x.shape for x in vectors])\n",
    "mean_vector = np.mean(vectors, axis=1)\n",
    "print(mean_vector)\n",
    "#extractor.save_to_csv(word)\n",
    "\n"
   ],
   "id": "39d4c22d41fa4ffb",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U7'), dtype('<U7'), dtype('<U14'))",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[104]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m vectors = \u001B[38;5;28mlist\u001B[39m(data[\u001B[33m'\u001B[39m\u001B[33mcow\u001B[39m\u001B[33m'\u001B[39m].values())\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m#print([x.shape for x in vectors])\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m mean_vector = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;28mprint\u001B[39m(mean_vector)\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m#extractor.save_to_csv(word)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Schreibtisch/Multiling/Masterarbeit/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3860\u001B[39m, in \u001B[36mmean\u001B[39m\u001B[34m(a, axis, dtype, out, keepdims, where)\u001B[39m\n\u001B[32m   3857\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3858\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m mean(axis=axis, dtype=dtype, out=out, **kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m3860\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_methods\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_mean\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3861\u001B[39m \u001B[43m                      \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Schreibtisch/Multiling/Masterarbeit/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:134\u001B[39m, in \u001B[36m_mean\u001B[39m\u001B[34m(a, axis, dtype, out, keepdims, where)\u001B[39m\n\u001B[32m    131\u001B[39m         dtype = mu.dtype(\u001B[33m'\u001B[39m\u001B[33mf4\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    132\u001B[39m         is_float16_result = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m ret = \u001B[43mumr_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, mu.ndarray):\n\u001B[32m    136\u001B[39m     ret = um.true_divide(\n\u001B[32m    137\u001B[39m             ret, rcount, out=ret, casting=\u001B[33m'\u001B[39m\u001B[33munsafe\u001B[39m\u001B[33m'\u001B[39m, subok=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mTypeError\u001B[39m: the resolved dtypes are not compatible with add.reduce. Resolved (dtype('<U7'), dtype('<U7'), dtype('<U14'))"
     ]
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b573d528247871d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
